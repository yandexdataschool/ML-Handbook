import Math from '../../../components/Math/Math';

Тут будет всякая полезная инфа про тексты

Рассмотрим задачу работы с текстом (например, классификации). Для простоты, предположим, что каждое слово уже представлено в виде некоторого вектора $x^k$, который некоторым образом описывает его. В простейшем случае каждому слову из словаря (множества всех известных слов) можно сопоставить его порядковый номер в словаре и сопоставить слову one-hot вектор с единицей на месте, соответствующем индексу слова. Более сложные способы представления слов и другой информации в векторном виде будут рассмотрены позднее.

Решение задачи можно разделить на два шага:

1. Представить текст (набор векторов $[x^0, \ldots, x^n]$) в виде вектора фиксированной размерности $h$.

   <Math block>

   $$
   h = e([x^0, \ldots, x^n]; \mathbf{\theta}_e).
   $$

   </Math>

2. Отобразить этот вектор в целевую переменную (метку класса/вектор вероятностей классов/...).

   <Math block>

   $$
   \hat{y} = d(h; \mathbf{\theta}_d).
   $$

   </Math>

При построении отображения $e$ необходимо учитывать, что различные тексты в обучающей выборке могут иметь различную длину, т.к. на выходе получается вектор фиксированной размерности. Простой вариант это сделать – посчитать некоторую статистику, например, _среднее значение_ векторов всех слов, попавших в текст. Это приведет к решению “Мешок слов” (Bag of Words).

![](./src/graph-for-text.png){: .center style="width:30vw"}

_Можно использовать и взвешенные линейные комбинации; одна из таких стратегий -- это использование TF-IDF_.

Но данные подходы не учитывают то, что **часть информации в тексте кроется не в самих словах, а именно в их порядке**. Рассмотрим простой пример: _“Студент сказал товарищу, что он не придет”_ и _“Студент не сказал товарищу, что он придет”_. Перестановка одного слова уже поменяла смысл предложения (а при вычислении среднего порядок всех слов игнорируется). Если бы модель могла учитывать порядок слов, это позволило бы извлечь больше информации из исходных данных, а значит, породить более информативное признаковое описание исходного текста.
Внимательный читатель обратит внимание, что здесь хорошо бы подошли марковские цепи.

Учесть наличие порядка в текста (как, в целом, и в любой другой последовательности) можно несколькими способами. Рассмотрим один из них.

![](./src/recurrent-encoder.png){: .center style="width:50vw"}

Для понимания принципов работы рекуррентных нейронных сетей, введем понятие **вектора контекста** $h \in \mathbb{H}$ _(вообще говоря, левого контекста или же контекста слева)_: контекстом будем называть представление об уже прочитанной части предложения.

Рассмотрим предложение “Сегодня утром жители города Н узрели необъяснимое атмосферное явление”. Будем читать его по словам: 0. На нулевом шаге мы не знаем ничего о предложении, которое мы рассматриваем. Контекст $h_0$ пуст.

1. На первом шаге мы видим слово “Сегодня”. Т.е. мы можем понять, что речь идет о дне написания предложения, но при этом не знаем ничего дополнительно. Контекст $h_1$ содержит информацию об этом.
2. Прочитав следующее слово, теперь нам доступна информация “Сегодня утром”, т.е. мы понимаем, что речь идет об утреннем времени суток дня написания, и она есть в контексте $h_2$.
3. Следующее слово позволяет нам узнать, что речь ведется о некоторых жителях утром дня написания – соответственно, она содержится в контексте $h_3$.
4. И т.д.

Каждая новая часть информации позволяет нам лучше понять, о чем именно ведется речь – _обогащает контекст_, т.е. делает наше представление более информативным.

Теперь необходимо сделать лишь одно допущение: **пусть контекст представим в виде вектора фиксированной размерности** (без ограничения общности, пусть это будет $32$). Т.е. независимо от длины прочитанного предложения, векторы $h_0$, $h_1$, $h_2$, $h_3$ и т.д. все имеют размерность $32$.

Чтобы научиться работать с текстами (и, вообще говоря, последовательностями переменной длины), необходимо для последовательности любой длины получать фиксированный вектор признакового описания. Для этих целей и можно использовать вектор контекста $h$. Остается лишь вопрос: как именно обновлять контекст? Структура этого обновления должна быть независимой от позиции в последовательности. Попробуем доработать пример выше.

На нулевом шаге у нас нет никаких предположений о рассматриваемом предложении. Ему соответствует вектор контекста $h_0$, например, полностью состоящий из нулей. Добавим технический токен (слово) начала предложения SOS (Start of sequence) и соответствующий ему вектор $x_0$.

Пусть на k-ом шаге по последовательности нам доступен вектор $x_k$, который соответствует слову, стоящему на этом месте) и вектор $h_{k-1}$, соответствующий контексту, который был получен на предыдущем шаге. Пусть размерность $x_k$ равна $16$, а размерность $h_{k-1}$ равна 32, как уже было обозначено ранее.
Необходимо ввести отображение $f$:

<Math block>

$$
f: \mathbb{X} \times \mathbb{H} \longrightarrow \mathbb{H},
$$

</Math>

<Math block>

$$
h_{k} = f(x_{k}, h_{k-1}; \mathbf{\theta}).
$$

</Math>

Вектор $x_{k}$ содержит новую информацию, которую мы бы хотели учесть при построении вектора $h_{k}$. Простая конкатенация веткоров $x_{k}$ и $h_{k}$ не подходит – она не сохраняет размерность вектора $h_{k}$. Можно воспользоваться линейной операцией:

<Math block>

$$
W[x_{k}, h_{k-1}] + b,
$$

</Math>

где $[]$ соответствует конкатенации вдоль оси признаков. Конечно, также стоит воспользоваться функцией активации, например, tanh. Т.е. итоговое отображение будет иметь вид:

<Math block>

$$
f(x_{k}, h_{k-1}; \mathbf{\theta}) = \tanh(W[x_{k}, h_{k-1}] + b),
$$

</Math>

<Math block>

$$
\mathbf{\theta} = \{W, b\}.
$$

</Math>

mark Данная операция проиллюстрирована на рисунке ниже.
#TBD: h\_{k-1} == z_k, там другая нумерация, а вместо tanh используется sigmoid.

_Стоит оговориться, что необходимо использовать подходящие функции активации. Т.к. результат применения данной операции f является входом аналогичной операции на следующем шаге, полезным будет наложение дополнительных ограничений на вектор h. Например, предположение, что все его компоненты принимают значения из $[-1;1]$._

_Таким образом мы построили простейший блок рекуррентной нейронной сети (также ее часто называют Vanilla RNN). На самом деле, в ней существует достаточно большое количество проблем (например, использование активаций tanh или sigmoid приводит к серьезному затуханию градиентов, отсутствие фильтрации информации и фиксированная размерность вектора контекста не позволяют выделять долгосрочные зависимости и др.) Более подробно эти вопросы будут рассмотрены в блоке по архитектурам для задач обработки естественного языка._

Также рекомендуем вам ознакомиться с [замечательной статьей за авторством Андрея Карпатого](http://karpathy.github.io/2015/05/21/rnn-effectiveness/)

mark Можно в дальнейшем добавить пример backpropagation’а для простейшей рекуррентной сети

Подводя промежуточный итог, стоит заметить, что мы рассмотрели достаточно простые способы работы с последовательностями. Фактически, на основе правильной формулировки ограничений и требований к решению мы построили простейшую рекуррентную нейронную сеть. Мы выделяем это отдельно, т.к. нейронные сети предоставляют огромную свободу действий в построении подходящих операций (и целых структур на их основе) для различных задач.
Более развернутый обзор архитектур, проблем и их решений в NLP доступен [здесь](https://gitlab.com/ysda_trove/ml-handbook/-/blob/neural_nets_nlp/chapters/neural_nets/nlp.md) и в замечательном [курсе по NLP](https://lena-voita.github.io/nlp_course.html) Лены Войта.
