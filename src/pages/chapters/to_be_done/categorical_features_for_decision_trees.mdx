### Регуляризация. **Критерии остановки (Pre-Pruning)**

Деревья решений могут очень точно приближать обучающую выборку. Можно построить дерево с нулевой ошибкой. Однако в общем случае такое дерево будет иметь сложную структуру и будет переобученным. При применении такая модель не будет показывать высокое качество. Для того, чтобы решить эту проблему, можно использовать регуляризацию. Есть две группы методов, которые позволяют этот сделать.

Первую группу методов объединяет то, что они применяются на этапе построения дерева.

Когда мы разбирали алгоритм построения решающего дерева, нам встречалось такое понятие, как критерий останова. Так называют критерий, в соотвествии с которым принимается решение о том, что разбиение в текущей вершине производить уже не нужно, и пришло время сформировать лист. Так можно предотвратить переобучение.

Перечислим несколько распространенных критериев останова.

- Достигнута выбранная максимальная глубина дерева от корня до данной вершины.
- Все объекты, дошедшие до вершины, относятся к одному классу.
- В дереве уже есть заданное количество листьев.
- При попытке разбиения в очередной вершине образуются множества мощности, не превышающей минимально допустимого числа объектов в листе.
- Не получается найти удачное в соответсвии с выбранной метрикой разбиение объектов на два множества.

Критерий останова может существенно повлиять на качество алгоритма. При неудачном выборе можно получить недообученную или пееобученную модель. При построении дерева нужно подбирать оптимальные значения для различных комбинаций параметров, а также принимать во внимание специфику решаемой задачи.

Также на этапе построения дерева можно использовать коэффициент регуляризации. Это своего рода штраф за сложность модели.

<<<<<<< develop
<<<<<<< develop
$$
R_{\alpha}(T) = R(T) + \alpha\left| \widetilde{T} \right|
$$
=======
<Math block>

=======
>>>>>>> fix: use MathLayout instead Math
$$
R_{\alpha}(T) = R(T) + \alpha\left| \widetilde{T} \right|
$$

<<<<<<< develop
</Math>
>>>>>>> feat: pass formulas to Math component

=======
>>>>>>> fix: use MathLayout instead Math
<!-- ![](images/Screen_Shot_2021-01-26_at_12.33.48.png) -->

$T$ - количество листьев в дереве ($R(T)$ это $H$)

Такой метод называется Minimal cost-complexity pruning. $\alpha \geq 0$ - это прпметр, который можно подбирать. От его выбора зависит качество модели.

Первая группа методов называется pre-pruning.

**Post-Pruning**

А можно ли что-то сделать, если дерево все-таки получилось переобученным?

В этом случае можно произвести процедуру, обратную построению дерева. Когда мы строили модель, мы, начиная с корня, в каждой вершине множество объектов, которые в нее попали, разбивали на два подмножества. А можно с тспользованием отложенной выборки, начиная с листьев, пробовать объединять подмножества в вершинах, имеющих общего родителя.

При построении дерева мы определяли момент, когда следует прекратить разбиение в вершине и пора сформировать лист. Теперь нужно следить за тем, не пора ли перестать объединять подмножества.

Однако стрижка деревьев на практике обычно не применяется. Процедура это достаточно сложная, и при этом особого смысла в ней нет, так как дерево решений обычно не используют в качестве самостоятельной модели. Деревья чаще всего соединяют в различные ансамбли. Например, в случайный лес или градиентный бустинг. Про эти модели вы узнаете далее в курсе. Дело в том, что при использовании ансамблей либо нет ничего страшного в том, что отдельное дерево получится переобученным, либо глубина деревьев изначально выбирается достаточно маленькой.

**Интерактивное построение алгоритма**

Комментарий к подразделу: добавить пример построения (маленького) дерева с определением оптимального разбиения в каждом листе, без критерия остановки/стрижки

**Алгоритмическая сложность построения дерева**

Мы разобрали алгоритм построения дерева решений и рассмотрели его составные части.

Давайте разберемся, какая у него вычислительная сложность.
Сложность алгоритма построения решающего дерева зависит от того, какой именно алгоритм используется, и как он реализован.

В общем случае при построении может получится произвольное дерево. Предположим, что дерево получилось сбалансированным.

Тогда высота дерева будет $\log_2(n)$

Очевидно, что в таком случае сложность предсказания будет $O(\log n)$

Разберемся, что будет при обучении модели.

Для примера рассмотрим задачу с вещественными значениями признаков. Допустим, получилось полное бинарное дерево, у которого последний уровень полностью заполнен, то есть все ответы располагаются в узлах этого уровня. Также предположим, что разбиение производилось до тех пор, пока в каждой вершине не окажется по одному объекту.

Граница, соответствующая оптимальному разбиению, будет расположена между минимальным и максимальным знаением признака. Сортировка значений признака для объектов поможет осуществить перебор вариантов разбиения более эффективно. Сортировка по $m$ фичам в каждой вершине занимает $O(n * m)$.

Осталось определить количество узлов, в которых будет выполнена эта операция.

Производиться она будет во всех узлах, кроме листьев.

Посчитаем количество таких вершин.

На каждом уровне будет вдвое больше вершин, чем на предыдущем. То есть для подсчета суммы можно воспользуется формулой суммы геометрической прогрессии.

$b1 = 1$;
$q = 2$;

Количество слагаемых в сумме вудет равно $\log_2(n)$. Например, если объектов 16, то есть на последнем уровне в дереве 16 вершин, то в сумме будут слагаемые:$1, 2, 4, 8$, их $\log_2(16) = 4$

Получим равенство:

<<<<<<< develop
<<<<<<< develop
$$
S\_{log_2(n)} = \frac{1 - q^{log_2(n)}}{1 - q}
$$
=======
<Math block>

=======
>>>>>>> fix: use MathLayout instead Math
$$
S\_{log_2(n)} = \frac{1 - q^{log_2(n)}}{1 - q}
$$

<<<<<<< develop
</Math>
>>>>>>> feat: pass formulas to Math component

=======
>>>>>>> fix: use MathLayout instead Math
Откуда получим, что операция сортировки будет производиться в $n - 1$ узле, что равно $O(n)$.

Таким образом, суммарная сложность рассмотренного алгоритма будет $O(mn^2\log n)$.

**Как построить дерево эффективно?**

Рассмотренный алгоритм на больших объемах данных будет работать достаточно долго. Хотелось бы его как-то ускорить.

Его самым слабым местом является сортировка данных. Если производить эту операцию в каждом узле дерева для каждой фичи, сложность получается $ O(m n logn) $

Одним из методов, позволяющих ускорить алгоритм, является использование гистограммного подхода. Идея метода состоит в разбиении данных на бины по каждой фиче.

Рассмотрим пример. Допустим, имеется датасет с двумя фичами.

![](images/hist.jpg)

При нахождении оптимального разбиения будем искать отсечку между бинами, а не между всеми объектам.

![](images/bins.png)

Такая оптимизация позволяет уменьшить сложность нахождения разбиения до $ O(m n_bins) Количество бинов много меньше количества данных в датасете, так что алгоритм, использующий эту идею будет работать намного быстрее.

Существуют 2 варианта реализации алгоритма, использующего такой метод: гистограмму можно строить один раз перед построением дерева, и в каждом узле. Оба варианта реализованы, например, в библиотеке xgboost. В зависимости от того, какой из вариантов выбран, и от количества бинов, качество модели будет различаться.

![](images/cmp.png)

Помимо того, что алгоритм с сортировкой всех данных работает довольно долго, он еще и тратит дополнительную память на хранение соответствия между отсортированными значениями для каждого признака и метками. Таким образом объем хранимых данных увеличивается примерно вдвое. При использовании гистограммного подхода этого недостатка нет.

Также на время работы алгоритма оказывают влияние количество фичей и количество примеров. Если это количество уменьшить, модель будет работать быстрее.

Для уменьшения количества данных можно просто брать случайные объекты, а можно проставить им веса в соответсвии с каким-то критерием, и выбирать наиболее важные объекты.

Количество фичей можно уменьшить, объединив некоторые из них по какому-то признаку.

Обе идеи, к примеру, используется в lightgbm. Они называются Gradient-based One-Side Sampling (GOSS) для сокращения количества объектов и Exclusive Feature Bundling (EFB) для уменьшения пространства признаков.

**Обработка пропусков**

Одним из преимуществ деревьев решений по сравнению с линейными моделями является возможность обрабатывать пропущенные значения признаков для объектов.

Конечно, можно заполнить пропуски нулем, средним значением, или какой-то другой подходящей для задачи величиной. Но существуют и другое подходы, применяющиеся для решения этой проблемы.

Например, можно сформировать для таких объектов новый класс в случае задачи классификации, или записать в качестве значения число, превосходящее значение признака для всей выборки в случае задачи регрессии. Это новое значение признака будет использоваться при определении оптимального разбиения в вершине.

Рассмотрим еще один подход. Допустим, при построении дерева на этапе определения лучшего предиката в какой-то вершине значение признака для объекта не известно. В этом случае вычислим значение критерия информативности без учета этого объекта.

Если оказалось так, что это разбиение оптимальное, отправим объект и в левое, и в правое поддерево с весами, пропорциональными доле объектов в этих поддеревьях. Таким образом, когда дерево построено, объект может находиться сразу в нескольких листьях с весами, сумма которых равна 1. Можно считать, что объект распределится по нескольким листьям. Эти веса нужно учитывать при определении ответа в листе.

Аналогично можно поступить на этапе применения модели. Если объект, для которого значение предиката в вершине не известно, также отправим его и в левое, и в правое поддерево с весами, пропорциональными доле объектов в этих поддеревьях. Итоговый прогноз для данного объекта будет определятся на основании прогнозов в каждом из листьев, в которые он попал с учетом весов.

Есть и другие варианты, как можно поступить с объектом в таком случае, помимо распределения с весами:

- Отправить его в ветвь, в которой больше объектов. То есть в наиболее вероятное поддерево.
- Случайно распределять такие объекты в соответствии с вероятностями, равными доле объектов в поддереве.

Еще один способ учета пропущенных значений - использование суррогатных предикатов. Идея метода состоит в использовании в вершине другого предиката, который дает похожее разбиение.

**Категориальные фичи.**

Еще одним преимуществом решающих деревьев является возможность учитывать категориальные признаки.

Для решения этой задачи также существуют различные методы. (Назвать их бы все по-нормальному и сделать подзаголовки)

_Построение n-арного дерева_

Например, можно в вершине разбивать выборку на столько частей, сколько существует различных значений категориального признака. Однако при таком подходе дерево может получится достаточно сложным.

_Разбиение на два подмножества_

Другая идея состоит в разбиении возможных значений категориального признака на два непересекающихся подмножества. Нужно определить предикат, который обеспечит наилучшее разбиение. Однако количество возможных разбиений будет $2^(n-1) - 1$. Разберемся, почему именно столько. Всего возможных подмножеств из $n$ элементов $2^n$. Однако два тривиальных варианта нас не интересует. Получим $2^n - 2$. Так как порядок множеств здесь неважен, получим $(2^n - 2) / 2$ = $2^(n-1) - 1$ вариант. Прийдется рассмотреть экспоненциальное от размера объектов число вариантов, что на практике почти никогда не применимо.

Однако в случае регрессии и бинарной классификации есть способ найти оптимальное разбиение быстрее, а именно за линейное время. Для этого преобразуем категориальные признаки в вещественные. Отсортируем возможные значения признака по некоторому критерию. Как именно сортировать значения рассмотрим далее.

Предположим, что значения отсортированы. Заменим значение i - ого элемента отсортированной последовательности на число i. В результате такого преобразования количество возможных разбиений станет равно i - 1.

Теперь разберемся, как можно отсортировать значения. Для начала рассмотрим задачу бинарной классификации.

$$
\sum\limits_{i \in X_m} \frac{[x_i^{(j)} = c_1][y_i = 1]}{\sum\limits_{i \in X_m} [x_i^{(j)} = c_1]} \leq \dots \leq
\sum\limits_{i \in X_m} \frac{[x_i^{(j)} = c_n][y_i = 1]}{\sum\limits_{i \in X_m} [x_i^{(j)} = c_n]}
$$

<!-- ![](images/Screen_Shot_2021-01-20_at_00.31.00.png) -->

В числителе каждой из дробей стоит произведение двух индикаторов: первый будет равен 1 в случае, если значение категориального признака равно c1, второй - если объект относится к 1 классу. Таким образом, i - ая дробь показывает, какая доля объектов, у который признак принимает значение ci, относится к 1 классу. Сортировка производится по возрастанию в соответствии со значением этой дроби.

В случае задачи регрессии поступим аналогично.

$$
\sum\limits_{i \in X_m} \frac{[x_i^{(j)} = c_1] y_i}{\sum\limits_{i \in X_m} [x_i^{(j)} = c_1]} \leq \dots \leq
\sum\limits_{i \in X_m} \frac{[x_i^{(j)} = c_n] y_i}{\sum\limits_{i \in X_m} [x_i^{(j)} = c_n]}
$$

<!-- ![](images/Screen_Shot_2021-01-20_at_00.41.56.png) -->

Теперь сортировка будет производиться по среднему значению целевой переменной.

Можно доказать, что при использовании этого подхода выбранное разбиение будет оптимальным. Таким же, как если бы мы перебирали всевозможные варианты разбиения объектов на 2 подмножества.

_Перенумерация категориальных признаков_

А почему бы не перенумеровать значения категориального признака? Например, есть признак Животное, и для него в обучающей выборке есть значения: Кошка, Собака, Хомяк, Обезьяна.

Тогда можно попробовать заменить Кошка - 0, Собака - 1, Хомяк - 2, Обезьяна - 3, и при построении дерева пробовать разбивать объекты в соответствии с этими числовыми значениями. Такой способ не работает. При задании соответствия искусственно задан порядок, и этот порядок не соответствует действительности. Например, получилось, что обезьяна ближе к хомяку, чем к кошке. Понять недостаток этого метода также помогает то, что при различных вариантах перенумерации получатся разные модели.

_One-hot encoding_

Давайте закодируем значения категориальной фичи бинарными векторами длины, равной количеству возможных значений признака. Для рассмотренного примера получится такое преобразование:

Кошка - 1000, Собака - 0100, Хомяк - 0010, Обезьяна - 0001

Вместо категориальной фичи с четырьмя значениями получили четыре бинарные фичи: является ли животное кошкой, собакой, хомяком, обезьяной.

Но если значений категориальной фичи очень много, признаков получится очень много, и строиться модель будет долго.

Идея, помогающая справиться с этой проблемой, состоит в отборе только n наиболее часто встречающихся значений. Это также помогает в борьбе с переобучением. Если значений фичи не много, модель не будет его принимать во внимание.

_Binary encoding_

Еще одним методом, помогающим решить проблему большой размерности пространства признаков, которая получается при применении one-hoe-encoding, является binary encoding. Идея метода в следующем: поставим в соответствие каждому значеию категориальной фичи натуральное число. Затем переведем это число в двоичную систему и будем использовать значения разрядов в качестве признаков. Размерность признакового пространства получится меньше, чем при использовании one-hoe-encoding

_Счетчики_

Также значения категориальных признаков можно заменить на счетчики. Например, на среднее значение целевой переменной при соответствующем значении. Однако использование этой идеи напрямую может привести к переобучению.

Если объектов для обучения достаточно много, то можно считать статистики по отложенной выборке. Это поможет справится с проблемой переобучения.

_Range features_

Допустим, в данных встречается такой признак, как возрастная категория (20-35 лет), или уровень дохода (100-150 тыс.руб) Что делать в таком случае?

Можно заменить значение такого признака на два новых признака: один из них будет равен левой границе диапазона, другой - правой.

Другой вариант - заменить диапазон его средним значением.

Целесообразность использования этих вариантов нужно оценить в зависимости от признака и от специфики задачи.

(Еще может написать про leave-one-out, leave-bucket-out, комбинацию значений, счетчики на префиксе в случайной перестановке, Backward Difference Encoding)

**Историческая справка: C4.5, CART, CHAID, Mars, как частные случаи**

Дописать

**Oblivious decision trees и другие виды деревьев**

Дописать
