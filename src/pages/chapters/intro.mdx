---
title: Введение
---

import Math from '../../components/Math/Math';

# Об этой книге

Эта книга написана коллективом добрых людей, состоящим из преподавателей и выпускников Школы анализа данных. Своим появлением она обязана обязана двум замечательным курсам. Во-первых, это [курс Константина Вячеславовича Воронцова](http://www.machinelearning.ru/wiki/index.php?title=%D0%9C%D0%B0%D1%88%D0%B8%D0%BD%D0%BD%D0%BE%D0%B5_%D0%BE%D0%B1%D1%83%D1%87%D0%B5%D0%BD%D0%B8%D0%B5_%28%D0%BA%D1%83%D1%80%D1%81_%D0%BB%D0%B5%D0%BA%D1%86%D0%B8%D0%B9%2C_%D0%9A.%D0%92.%D0%92%D0%BE%D1%80%D0%BE%D0%BD%D1%86%D0%BE%D0%B2%29) , на котором выросло подавляющее большинство и авторов книги, да и вообще ML-специалистов в России. Во-вторых, это курс [NLP Course | For You](https://lena-voita.github.io/nlp_course.html) Лены Войта, благодаря которому мы поняли, как должен выглядеть современный учебник, и на который мы будем регулярно ссылаться в частях, связанных с анализом текста.

Идея была такая: записать сложившийся в ШАДе курс машинного обучения в виде книги, при этом избежав каких-либо компромиссов: нигде ничего не упрощать чрезмерно, дать всю возможную теорию, описать и исторически важные алгоритмы, и применяющиеся сегодня, вместе с теорией рассказывать и практические вопросы о реализации алгоритмов и работе с данными.

Мы считаем, что для чтения книги вы совершенно обязательно должны уверенно владеть линейной алгеброй, математическим анализом, теорией вероятностей и уметь программировать на хотя бы одном каком-нибудь языке (а желательно на Python). Знания статистики и методов выпуклой оптимизации не обязательны, хотя сделают чтение комфортнее.

Читая книгу, вы обязательно заметите в ней ошибки, неточности и плохо объяснённые детали. В таком случае, пожалуйста, не поленитесь прислать нам merge-реквест с исправлениями ([сюда](https://gitlab.com/ysda_trove/ml-handbook)) – так вы поможете следующим поколениям студентов, и вам самим тоже будет приятно сознавать, какой вы молодец.

Итак, приступим.

# Машинное обучение

**_Машинное обучение_ --- это наука, изучающая алгоритмы, автоматически улучшающиеся благодаря опыту.**

С момента возникновения компьютеров человечество пытается автоматизировать все больше и больше задач. Многие проблемы получается решить с помощью программирования, но по разным причинам выходит это не всегда. Например, бывают задачи, которые люди не могут решить сами; более того, их доказуемо нельзя эффективно решить, и компьютер здесь чуда тоже не совершит (речь об NP-трудных задачах). Но бывают и задачи, которые для людей особенного труда не составляют, но которые почему-то трудно или вообще невозможно запрограммировать, например:

- Перевести текст с одного языка на другой;

- Продиагностировать болезнь по симптомам;

- Сравнить, какой из двух документов в интернете лучше подходит под данную поисковую фразу;

- Сказать, что изображено на картинке;

- Оценить, по какой цене удастся продать квартиру.

Эти задачи обьединяет как минимум несколько вещей. Во-первых, их решение можно записать как функцию, которая отображает **объекты** или **примеры** в **предсказания** (например, больных в диагнозы, документы в оценку релевантности). Во-вторых, нам подойдет не идеальное, а достаточно хорошее решение: ведь и доктора иногда делают ошибки в диагнозах, и вы не всегда можете сказать, что же именно изображено на картинке. В-третьих, у нас есть много примеров правильных ответов (скажем, переводов предложения на другой язык или подписей к заданной картинке), а примеры неправильных ответов как правило не составляет труда сконструировать. Функция, отображающая объекты в предсказания, именуется **моделью** (или ещё **алгоритмом**), а набор примеров иногда ещё называют **обучающей выборкой** или **датасетом** (от слова dataset). Обучающая выборка состоит из описания примеров и известных ответов для них (истинные ответы иногда еще называют таргетами, от target).

Модель, как мы только что договорились, принимает на вход примеры, но так как мы хотим, чтобы она жила в компьютере, примеры эти надо описывать каким-то дружелюбным к этому самому компьютеру способом. В идеальном мире объекты описываются с помощью набора вещественных признаков, то есть вектора чисел, где на $j$-том месте стоит число из $\mathbb{R}$, которое может так или иначе описывать наш обьект. Это может быть, например, количество комнат в квартире, или бинарный индикатор того, что у больного есть одышка, или яркость пикселя номер 23467 на картинке. Признаковые описания обьектов традиционно обозначаются $x \in \mathbb{R}^d$, а весь датасет примеров задается как матрица $X$, где каждая строка это описание одного примера, а всего строк $n$.

По имеющемуся у нас на руках набору примеров:

- **объектов** (это могут быть скачанные из интернета картинки, истории известных нам больных, активность пользователей на сервисе)

- и **ответов** для них (это могут быть подписи к картинкам, диагнозы, факты ухода пользователей с сервиса), которые мы также будем иногда называть таргетами

мы хотим найти модель, предсказания которой достаточно хороши. Но что значит "достаточно хороши"? Обычно "хорошесть" измеряют с помощью **метрик**, то есть функций, которые показывают, насколько сильно ответы модели похожи на ответы, данные нам в примерах.

- для задачи с диагнозами хорошими метриками могут быть, например, доля правильно поставленных диагнозов или доля заражённых, которым удалось поставить правильный диагноз (а вы поняли разницу?);

- в задаче с ценой квартиры -- доля квартир, для которых разница между предсказанным и истинным значением цены не превысила какого-то порога, или средний модуль разницы между предсказанным и истинным значением;

- в задаче с поисковыми документами -- доля пар документов, которые мы упорядочили неправильно. Чем меньше значение метрики, тем лучше работает наша модель. Решение задачи ML – это поиск такой модели, которая минимизирует значение метрики на датасете.

Как найти функцию, которая минимизирует метрику на данной выборке? В общем случае решить эту задачу не представляется возможным. Но, быть может, если ограничиться каким-то классом функций, то на нём получится найти оптимум?

Для примера давайте возьмём задачу предсказания цены квартиры, в качестве класса моделей – константные функции $f(x) = c$ (то есть будем для всех квартир предсказывать одно и то же значение цены), а в качестве метрики – **среднее абсолютное отклонение** (mean absolute error, она же **MAE**)

<<<<<<< develop
$$
MAE(y) = L(y) = \frac1n\sum\limits_{i=1}^n |f(x_i) - y_i| \rightarrow \min\limits_f
$$
=======
<Math block>

$$
MAE(y) = L(y) = \frac1n\sum\limits_{i=1}^n |f(x_i) - y_i| \rightarrow \min\limits_f
$$

</Math >
>>>>>>> feat: pass formulas to Math component

где $f$ -- это модель (та самая, $f(x) = c$), $x_1,\ldots,x_n$ -- обучающие примеры (те данные о квартирах, которые мы смогли достать), $y_1,\ldots,y_n$ -- правильные ответы (то есть цены на известные нам квартиры). Так как предсказание модели константное, по нему легко можно взять производную, которую мы приравняем к нулю, чтобы найти оптимальное значение $c$:

<<<<<<< develop
$$
\frac{\mathcal{d} L(y)}{\mathcal{d}c} = \frac1n\sum\limits_{i=1}^n sign(c - y_i) = 0.
$$
=======
<Math block>

$$
\frac{\mathcal{d} L(y)}{\mathcal{d}c} = \frac1n\sum\limits_{i=1}^n sign(c - y_i) = 0.
$$

</Math>
>>>>>>> feat: pass formulas to Math component

Нетрудно увидеть, что 0 (и, соответственно, оптимум нашей метрики) достигается в точке $f(x) = \mathrm{median}(y)$.

Прекрасно, значит, в классе константных функций мы можем найти оптимальную модель, но, может быть, это можно сделать и в каком-нибудь более интересном классе? Этому вопросу и будет посвящена большая часть нашей книги. Классический курс ML состоит из описания классов моделей и способов работы с ними. Несмотря на то, что для решения большинства практических задач на сегодня достаточно знать только два типа моделей – **градиентный бустинг на решающих деревьях** и **нейросетевые модели** – мы постараемся рассказать и про другие, чтобы развить у вас глубинное понимание предмета и дать возможность не только использовать лучшие сложившиеся практики, но и, при вашем желании, участвовать в проработке новых идей и поиске новых методов, уже в роли исследователя, а не просто инженера.

Не любое сочетание задач, моделей и метрик имеет смысл. Скажем, если вы предсказываете класс опасности вещества (бывает от 1-го до 4-го) по его химической формуле, MAE по умолчанию представляется не очень обоснованной метрикой, уж точно менее удачной, чем доля правильных предсказаний: ведь несмотря на то, что классы вроде как представлены целыми числами, их порядок не имеет чёткой семантики. И наоборот, если мы предсказываем возраст человека в годах по его фотографии, точность предсказаний кажется уже не столь удачной метрикой, а вот MAE – вполне. В ходе курса вы научитесь выбирать правильные метрики для различных задач и классов моделей – и поймёте, что этот выбор не всегда так однозначен, как хотелось бы.

При этом стоит отметить, что при постановке бизнес-задачи метрика обычно выбирается из соображений целесообразности, а не из-за её совместимости с какими-то моделями (больше того, модель обычно возникает уже в процессе как средство). Например, человек, ставящий аналитику задачу о предсказании цены квартиры, может попросить в качестве метрики использовать доход риэлторского агенства или аудиторию сайта этого агенства. Ясно, что в такой ситуации особо ничего не продифференцируешь, потому что доход можно лишь неявно связать с качеством предсказания цен на квартиры. Или же метрика окажется принципиально не дифференцируемой (как, например, точность предсказания). Что же делать? Обычно поступают так: выбирают прокси метрику, оптимизируя которую мы будем улучшать и исходную метрику или хотя бы уповать на это (например, можно таки использовать метрику абсолютной близости и понадеяться, что точные предсказания приведут к успеху и популярности компании). В таком случае метрику, которую используют при поиске оптимальной модели, называют **функцией потерь**, ошибкой, лоссом (от loss) или лосс-функцией. Выбор удачной функции потерь и класса моделей для конкретной задачи – тонкое и хорошо оплачиваемое искусство, которому мы надеемся научить вас в нашем курсе.

## Виды задач

Описанные выше задачи являются примерами **обучения с учителем** (supervised learning), так как правильные ответы для каждого объекта обучающей выборки заранее известны.
Задачи обучения с учителем в свою очередь делятся на следующие виды в зависимости от того, каким может быть множество $\mathbb{Y}$ всех возможных ответов (таргетов):

1. &nbsp;$\mathbb{Y} = \mathbb{R}$ или $\mathbb{Y} = \mathbb{R}^m$ – регрессия. Примерами задач регрессии является предсказание продолжительности поездки на каршеринге, спрос на конкретный товар в конкретный день или погода в вашем городе на завтра (температура, влажность и давление – это пример, когда мы предсказываем сразу несколько вещественных чисел).
2. &nbsp;$\mathbb{Y} = \{0, 1\}$ – бинарная классификация. Например, мы можем предсказывать, кликнет ли пользователь по рекламному объявлению, вернет ли клиент кредит в установленный срок, сдаст ли студент сессию, случится ли определенное заболевание с пациентом (на основе, скажем, его генома), есть ли на картинке банан.
3. &nbsp;$\mathbb{Y} = \{1, \dots, K\}$ – многоклассовая (multi-class) классификация. Например, определение предметной области для научной статьи (математика, биология, психология и т.д.).
4. &nbsp;$\mathbb{Y} = \{0, 1\}^K$ – многоклассовая классификация с пересекающимися классами (multi-label classification). Например, задача автоматического проставления тегов для ресторанов (логично, что ресторан может одновременно иметь несколько тегов).
5. &nbsp;$\mathbb{Y}$ – конечное упорядоченное множество – ранжирование. Основным примером является задача ранжирования поисковой выдачи, где для любого запроса нужно отсортировать все возможные документы по релевантности этому запросу; при этом оценка релевантности имеет смысл только в контексте сравнения двух документов между собой, её абсолютное значение информации не несёт.

Помимо обучения с учителем, есть и **обучение без учителя** (**unsupervised learning**) – задачи, для которых нам известны только данные, а ответы неизвестны или вообще не существуют, и их поиск не является самоцелью. Классическим примером алгоритма обучения без учителя является кластеризация – задача разделения объектов на группы, обладающие некоторыми (неизвестными нам, но, как мы в глубине души надеемся, интерпретируемыми) свойствами. Примером может служить кластеризация документов из электронной библиотеки по темам или кластеризация абонентов мобильного оператора. Другое применение кластеризации – сегментация пользователей, то есть выделение групп пользователей со схожими паттернами поведения. Объединив схожих по интересам пользователей, мы сможем делать более выгодные для них предложения; при этом изначально мы не знаем ни сколько будет этих групп, ни чем они могут характеризоваться – обнаружение различных паттернов поведения является потенциальной пользой от кластеризации.

Но неправильно думать о supervised/unsupervised, как о непересекающихся классах; это скорее полюса, между которыми находятся многие задачи, которые нельзя в полной мере отнести ни к одному из них, такие как, например:

1. Порождение новых объектов – задача генерации правдоподобных объектов. На первый взгляд может показаться, что таким моделям сложно найти применение: понятно, почему мы хотим научиться отличать изображение кошки от изображения собаки, но не очень понятно, зачем нам уметь генерировать изображения собак. Однако с помощью такой модели также можно научиться увеличивать разрешение изображения и применять любимые всеми маски в Snapchat и Instagram.
2. Обучение представлений – задача построения компактных векторов небольшой размерности из сложных по структуре данных (например, изображений, звука, текстов, графов) так, чтобы близкие по структуре или семантике данные получали метрически близкие представления.

Бывают и другие виды (и даже парадигмы) машинного обучения; так что если вы встретите задачу, которую никак не получается отнести к одному из перечисленных выше типов, не расстраивайтесь и знайте, что где-то дальше в учебнике вас ждёт рассказ про такие задачи.

## Выбор алгоритма ML, переобучение

Заметим, что мы вас обманули: очевидно, что для любой ML задачи можно построить идеальную модель – надо всего лишь запомнить всю обучающую выборку с ответами. Такой классификатор может достичь идеального качества по любой метрике, но радости от него довольно мало, ведь мы хотим, чтобы он выявил какие-то закономерности в данных и помог нам с ответами там, где мы их не знаем. Для того, чтобы предохранить себя от такого конфуза, поступают обычно так: делят выборку с данными на две части: _обучающую выборку_ и _тестовую выборку_ (_train_ и _test_). Обучающую выборку используют для, собственно, обучения модели, а метрики считают на тестовой.

Такой подход позволяет отделить модели, которые просто удачно подстроились к обучающим данным, от моделей, в которых произошла _генерализация_ (generalization), то есть они на самом деле кое-что поняли о том, как устроены данные и могут выдавать полезные предсказания для обьектов, которые не видели.

Например, рассмотрим три модели регрессионной зависимости уровня счастья людей от их доходов, построенные на одном и том же датасете. Отложим на графике их ошибки (пока не очень важно, метрики или функции потерь) на тестовой и обучающей выборке:
![overfit](https://qph.fs.quoracdn.net/main-qimg-17ec84ff3f63f77f6b368f0eb6ef1890)

С увеличением сложности модели ошибка на обучающей выборке падает. Очень сложная модель будет работать примерно так же, как модель, просто запомнившая всю обучающую выборку, но с генерализацией всё будет плохо: ведь выученные закономерности будут слишком специфическими, подогнанными под то, что происходит на обучающей выборке. Мы видим это на трёх графиках сверху: линейная функция очень проста, но и закономерность приближает лишь очень грубо; на правом же графике мы видим довольно хитрую функцию, которая точно подобрана под значения из обучающей выборки, но явно слишком эксцентрична, чтобы соответствовать какой-то природной зависимости. Оптимальная же генерализация достигается на модели не слишком сложной и не слишком простой.

Алгоритм, избыточно подстроившийся под данные, называют **переобученным**. Способность же алгоритма к генерализации, то есть к предоставлению адекватных предсказаний вне обучающей выборки, называют **обобщающей способностью**.

Точный способ выбрать алгоритм оптимальной сложности по данной задаче нам пока неизвестен, хотя какую-то теоретическую базу имеющимся философским наблюдениям мы дадим в главе про теорию обучения; при этом есть хорошо продуманная методология сравнения разных моделей и выбора среди них оптимальной – об этом мы обязательно расскажем вам в следующих главах. А пока дадим самый простой и неизменно ценный совет: не забывайте считать метрики на тестовой выборке и никогда не смешивайте её с обучающей!

## Что же такое модель

В курсе мы будем активно использовать слово "модель", и нужно заранее договориться о том, что мы имеем в виду, чтобы вам не казалось, что мы одним и тем же словом называем совсем разные вещи.

Мир, в котором мы живём, бесконечно сложен, и вряд ли у нас есть шанс хоть что-то измерить и предсказать совершенно точно, будь то форма Земли или вероятность того, что пользователь останется доволен поисковой выдачей. Но как-то работать нужно, и поэтому люди вводят (упрощённые) описания мироздания, которые и называют моделями.

Например, "Земля плоская" – это модель, и не такая плохая, как вам может показаться. Ей активно пользуются, когда всё происходит в масштабах одного города и кривизной поверхности можно пренебрегать. С другой стороны, если мы попробуем рассчитать кратчайший путь из Парижа в Лос-Анджелес, модель плоской Земли выдаст неадекватный ответ, она войдёт в противоречие с имеющимися данными, и её придётся заменить на "Земля круглая", "Земля имеет форму эллипсоида" и так далее – в той мере, в которой нам важна точность и в какой нам это позволяет (не)совершенство измерительной техники. Так, модель "Земля – это безымянная штука с шершавостями на месте горных хребтов" очень точная и замечательная, но, возможно, будет избыточно сложной для большинства практических задач и при этом слишком тяжёлой в плане вычислений.

То же самое верно и для машинного обучения. Если мы предсказываем цену квартиры, мы не можем учесть всех на свете тонкостей и сделать это совершенно точно. С одной стороны, нам фантазии не хватит, чтобы придумать все на свете факторы, влияющие на цену (да и вообще в природу на квантовом уровне внесена случайность). С другой – данных у нас конечное количество и, скажем, если мы даже знаем, что квартира, хозяин которой ходит с эльфийскими ушами и по ночам играет на арфе, продаётся за большие деньги, у нас просто нету другой квартиры, которая отличалась бы от этой лишь ушами хозяина, чтобы сделать хоть сколь-нибудь ответственный вывод о том, что именно уши повлияли на цену. Поэтому мы с самого начала должны примириться с тем, что мы ищем не истинную зависимость, а лишь приближённую – то есть строим модель. Например, такую: "цена квартиры линейно зависит от метража и логарифмически от расстояния до ближайшего метро". Или такую: "цена квартиры линейно зависит от суммарного метража и как многочлен второй степени от метража ванных комнат". Или вот такую: "в некоторых случаях цена квартиры линейно зависит от метража и логарифмически от расстояния до ближайшего метро, а в некоторых она просто равна константе". В каждой модели у нас есть **обучаемые параметры** (в данном случае коэффициенты зависимостей, а у третьей модели ещё и правило, разделяющее одни случаи и другие), которые мы будем оптимизировать в ходе обучения. Кроме того, нам придётся научиться сравнивать между собой разные модели.

Но, как мы видели в примере с формой Земли, не все модели являются предсказательными. Например, мы можем задуматься о том, как устроена вся совокупность квартир (виденных нами, не виденных нами и не существующих пока, но правдоподобных). "Половина квартир – двухкомнатные" – это тоже модель, и она, как мы увидим, может оказаться полезной для построения уже предсказательной модели цены, равно как и модель "Расстояние от квартиры до ближайшего метро имеет нормальное распределение". Больше того, и для цены мы можем давать не точный ответ, а что-то в духе "ну, цена этой квартиры заключена между такими-то значениями" или "цена этой квартиры имеет нормальное распределение с такими-то параметрами, то есть вот такое значение, конечно, вероятнее всего, но вообще, мы не очень уверены, и вот так можно выразить нашу неуверенность". Модели данных вы можете применить и для того, чтобы генерировать новые объекты – и если генерировать квартиры с ценами звучит, как сомнительное занятие, то генерация осмысленного текста или аниме точно заслуживает право на существование. Наконец, понимание того, как устроены данные, может оказаться полезным для поиска аномалий: так, если какая-то квартира уж совсем никак не соответствует модели (скажем, если это землянка по цене в три раза выше средней по рынку или если в ней на две жилых комнаты десять ванных комнат), у нас есть повод заподозрить шутников или мошенников (но в то же время это может означать и то, что мы просто недостаточно данных собрали, и модель в процессе обучения просто не повидала подобного).

В следующих главах вы узнаете, что модели машинного обучения бывают очень разными, что они могут служить разными целям и к ним могут предъявляться нетривиальные требования.

## Практические вопросы: данные для модели

Как мы уже выяснили, модель машинного обучения – это функция, восстанавливающая зависимость в данных, а значит и её построение должно начинаться с данных.

Будьте готовы к тому, что вам не принесут обучающий датасет на блюдечке с золотой каёмочкой: процесс его добывания и очистки – это долгая, мучительная, но благодарная работа, ведь от того, как много примеров вам доступно и насколько хорошо они отобраны, напрямую зависит качество решения задачи.

Представьте, что, вооружившись знаниями о моделях машинного обучения, вы решили сделать аналог распределительной шляпы для ШАДа: построить модель, которая по анкете абитуриента сможет предсказать, какое направление ему стоит выбрать. Первый же вопрос, который не должен застать врасплох: где найти обучающие примеры для модели? Хватит ли нам исторических данных о студентах ШАДа (которых всего-то пара тысяч) для построения надёжного правила?

Много проблем с количеством данных бывает у медиков: в конкретной больнице пациентов с конкретным заболеванием обычно маловато, а коллеги из других больниц редко делятся данными. Создателям моделей для работы с видео (например, приложений автоматического увеличения разрешения) может быть непросто, потому что не так-то много качественного видео лежит в открытом доступе; при этом если брать что попало, можно столкнуться с юридическими проблемами, а если учиться на фильмах эпохи немого кино (которые уже в открытом доступе), то выйдет скверно, так как вы будете применять свой инструмент к совсем другим данным.

Чем сложнее модель, тем больше зависимостей в данных она может выделить, но тем и больше обучающих примеров необходимо для её обучения. Поэтому если в процессе решения задачи вы собираетесь прибегнуть к нейросетевым подходам, будьте готовы к тому, что вам нужно будет собрать (и разметить: вы удивитесь, как часто людям не хватает для счастья каких-то там десяти тысяч картинок с обведёнными на них котиками) очень много данных.

Когда данные вроде как собраны, начинается предобработка: ведь нельзя просто так засунуть ужасные, кривые, полные треша логи в нейронную сеть.

Важно следить не только за количеством данных, но и за их качеством. Так, нередко в датасетах встречаются выбросы – объекты, находящиеся далеко от остальных наблюдений. Важно понимать, что это могут быть как просто уникальные объекты (например, во всей выборке средняя температура в Москве в июле 18 градусов, а в июле 2010 – 26 градусов), так и ошибки (в выборке может встретиться значение температуры 64, что должно навести вас на подозрения).

![](https://cdn-images-1.medium.com/max/800/0*zga8uYaZR6YU3JqH.png)

Некоторые признаки, описывающие ваши объекты, могут оказаться шумовыми: они никак не помогут решить вашу задачу. Например, признак "время года во время взятия анализа" вряд ли поможет вам определить наличие опухоли у пациента.
Некоторые модели плохо справляются с данными, в которых какие-то измерения пропущены (особенно часто эта проблема возникает в медицинских задачах: у части больных нет одних анализов, у части – ещё каких-то), другие плохо работают с признаками в разных масштабах.

Методы борьбы с этими и многими другими проблемами мы осветим в главе про данные, однако неплохой универсальный совет таков: смотрите на данные (про то, как это лучше делать, мы тоже обязательно расскажем).

Остановимся чуть поподробнее на признаках (или фичах, от слова features) объектов. Они бывают:

- бинарными (например, признак "наличие одышки" в задаче предсказания пневмонии)
- вещественными (например, "жилая площадь" в задаче предсказания спроса на квартиру)
- категориальными: принимают значения из неупорядоченного множества ("тип ресторана: фудкорт, отдельное кафе, еда с собой" в задаче предсказания выручки точки питания)
- ординальными: принимают значения из упорядоченного множества ("класс по доходу: бедный/богатый, средний класс" в задаче рекомендации товаров); этот вид признака можно спутать с категориальным, однако в данном случае порядок значений важен (кроме того, разница между бедным и средним классом может быть совсем не такой же, как разница между средним и богатым, так что, наверное, не стоит просто так делать этот признак вещественным)

Признаки могут иметь и более сложную внутреннюю структуру, работа с которой может потребовать дополнительных усилий. Так, в качестве признака для конкретного человека в задаче предсказания его годового дохода может служить фотография. Разумеется, фотографию можно представить и как некоторое количество бинарных или вещественных признаков, каждый из которых кодирует соответствующий пиксель изображения – но получится чертовски много признаков, многие из которых бессмысленны, скоррелированы и так далее. Поэтому полезно вместо фотографии использовать её сжатое векторное представление, сохраняющее семантику (сейчас обычно их получают с помощью предобученных нейросетей). Точно так же слова и даже тексты можно превращать в векторы (с помощью word2vec, GloVe и так далее) и уже дальше работать с ними в обычном режиме. В последующих главах мы покажем, как работать с самыми разными типами данных, делая из них что-то понятное и пригодное для того, чтобы покормить вашу любимую модель машинного обучения.

## Практические вопросы: а что ещё?

Разумеется, подготовкой данных дело не кончается. Нужно и выбирать модели, и обучать их, и сравнивать, и как-то понимать, что полученная модель выдаёт адекватное качество. Всё это вас ждёт на страницах учебника. И если в теоретической ветке мы будем знакомить вас с различными моделями и подходами машинного обучения, со спецификой конкретных задач, то в практической вы будете познавать, как устроены различные этапы решения задачи и как собрать их из отдельных лоскутков в один большой, красивый и логичный пайплайн.
