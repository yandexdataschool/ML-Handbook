---
title: Введение
---

# Об этой книге

Эта книга написана коллективом добрых людей, состоящим из преподавателей и выпускников Школы анализа данных. Своим появлением она обязана обязана двум замечательным курсам. Во-первых, это [курс Константина Вячеславовича Воронцова](http://www.machinelearning.ru/wiki/index.php?title=%D0%9C%D0%B0%D1%88%D0%B8%D0%BD%D0%BD%D0%BE%D0%B5_%D0%BE%D0%B1%D1%83%D1%87%D0%B5%D0%BD%D0%B8%D0%B5_%28%D0%BA%D1%83%D1%80%D1%81_%D0%BB%D0%B5%D0%BA%D1%86%D0%B8%D0%B9%2C_%D0%9A.%D0%92.%D0%92%D0%BE%D1%80%D0%BE%D0%BD%D1%86%D0%BE%D0%B2%29) , на котором выросло подавляющее большинство и авторов книги, да и вообще ML-специалистов в России. Во-вторых, это курс [NLP Course | For You](https://lena-voita.github.io/nlp_course.html) Лены Войта, благодаря которому мы поняли, как должен выглядеть современный учебник, и на который мы будем регулярно ссылаться в частях, связанных с анализом текста.

Идея была такая: записать сложившийся в ШАДе курс машинного обучения в виде книги, при этом избежав каких-либо компромиссов: нигде ничего не упрощать чрезмерно, дать всю возможную теорию, описать и исторически важные алгоритмы, и применяющиеся сегодня, вместе с теорией рассказывать и практические вопросы о реализации алгоритмов и работе с данными.

Мы считаем, что для чтения книги вы совершенно обязательно должны уверенно владеть линейной алгеброй, математическим анализом, теорией вероятностей и уметь программировать на хотя бы одном каком-нибудь языке (а желательно на Python). Знания статистики и методов выпуклой оптимизации не обязательны, хотя сделают чтение комфортнее.

Читая книгу, вы обязательно заметите в ней ошибки, неточности и плохо объяснённые детали. В таком случае, пожалуйста, не поленитесь прислать нам merge-реквест с исправлениями ([сюда](https://gitlab.com/ysda_trove/ml-handbook)) – так вы поможете следующим поколениям студентов, и вам самим тоже будет приятно сознавать, какой вы молодец.

Итак, приступим.

# Машинное обучение

**_Машинное обучение_ --- это наука, изучающая алгоритмы, автоматически улучшающиеся благодаря опыту.**

С момента возникновения компьютеров человечество пытается автоматизировать все больше и больше задач. Многие проблемы получается решить с помощью программирования, но по разным причинам выходит это не всегда. Например, бывают задачи, которые люди не могут решить сами; более того, их доказуемо нельзя эффективно решить, и компьютер здесь чуда тоже не совершит (речь об NP-трудных задачах). Но бывают и задачи, которые для людей особенного труда не составляют, но которые почему-то трудно или вообще невозможно запрограммировать, например:

- Перевести текст с одного языка на другой;

- Продиагностировать болезнь по симптомам;

- Сравнить, какой из двух документов в интернете лучше подходит под данную поисковую фразу;

- Сказать, что изображено на картинке;

- Оценить, по какой цене удастся продать квартиру.

Эти задачи обьединяет как минимум несколько вещей. Во-первых, их решение можно записать как функцию, которая отображает **объекты** или **примеры** в **предсказания** (например, больных в диагнозы, документы в оценку релевантности). Во-вторых, нам подойдет не идеальное, а достаточно хорошее решение: ведь и доктора иногда делают ошибки в диагнозах, и вы не всегда можете сказать, что же именно изображено на картинке. В-третьих, у нас есть много примеров правильных ответов (скажем, переводов предложения на другой язык или подписей к заданной картинке), а примеры неправильных ответов как правило не составляет труда сконструировать. Функция, отображающая объекты в предсказания, именуется **моделью** (или ещё **алгоритмом**), а набор примеров иногда ещё называют **обучающей выборкой** или **датасетом** (от слова dataset). Обучающая выборка состоит из описания примеров и известных ответов для них (истинные ответы иногда еще называют таргетами, от target).

Модель, как мы только что договорились, принимает на вход примеры, но так как мы хотим, чтобы она жила в компьютере, примеры эти надо описывать каким-то дружелюбным к этому самому компьютеру способом. В идеальном мире объекты описываются с помощью набора вещественных признаков, то есть вектора чисел, где на $$j$$-том месте стоит число из $$\mathbb{R}$$, которое может так или иначе описывать наш обьект. Это может быть, например, количество комнат в квартире, или бинарный индикатор того, что у больного есть одышка, или яркость пикселя номер 23467 на картинке. Признаковые описания обьектов традиционно обозначаются $$x \in \mathbb{R}^d$$, а весь датасет примеров задается как матрица $$X$$, где каждая строка это описание одного примера, а всего строк $n$.

По имеющемуся у нас на руках набору примеров:

- **объектов** (это могут быть скачанные из интернета картинки, истории известных нам больных, активность пользователей на сервисе)

- и **ответов** для них (это могут быть подписи к картинкам, диагнозы, факты ухода пользователей с сервиса), которые мы также будем иногда называть таргетами

мы хотим найти модель, предсказания которой достаточно хороши. Но что значит "достаточно хороши"? Обычно "хорошесть" измеряют с помощью **метрик**, то есть функций, которые показывают, насколько сильно ответы модели похожи на ответы, данные нам в примерах.

- для задачи с диагнозами хорошими метриками могут быть, например, доля правильно поставленных диагнозов или доля заражённых, которым удалось поставить правильный диагноз (а вы поняли разницу?);

- в задаче с ценой квартиры -- доля квартир, для которых разница между предсказанным и истинным значением цены не превысила какого-то порога, или средний модуль разницы между предсказанным и истинным значением;

- в задаче с поисковыми документами -- доля пар документов, которые мы упорядочили неправильно. Чем меньше значение метрики, тем лучше работает наша модель. Решение задачи ML – это поиск такой модели, которая минимизирует значение метрики на датасете.

Как найти функцию, которая минимизирует метрику на данной выборке? В общем случае решить эту задачу не представляется возможным. Но, быть может, если ограничиться каким-то классом функций, то на нём получится найти оптимум?
Для примера давайте возьмём задачу предсказания цены квартиры, в качестве класса моделей – константные функции $$f(x) = c$$ (то есть будем для всех квартир предсказывать одно и то же значение цены), а в качестве метрики – **среднее абсолютное отклонение** (mean absolute error, она же **MAE**)

$$MAE(y) = L(y) = \frac1n\sum\limits_{i=1}^n |f(x_i) - y_i| \rightarrow \min\limits_f$$

где $$f$$ -- это модель (та самая, $$f(x) = c$$), $$x_1,\ldots,x_n$$ -- обучающие примеры (те данные о квартирах, которые мы смогли достать), $$y_1,\ldots,y_n$$ -- правильные ответы (то есть цены на известные нам квартиры). Так как предсказание модели константное, по нему легко можно взять производную, которую мы приравняем к нулю, чтобы найти оптимальное значение $$c$$:

$$\frac{\mathcal{d} L(y)}{\mathcal{d}c} = \frac1n\sum\limits_{i=1}^n sign(c - y_i) = 0.$$

Нетрудно увидеть, что 0 (и, соответственно, оптимум нашей метрики) достигается в точке $$f(x) = \mathrm{median}(y)$$.

Прекрасно, значит, в классе константных функций мы можем найти оптимальную модель, но, может быть, это можно сделать и в каком-нибудь более интересном классе? Этому вопросу и будет посвящена большая часть нашей книги. Классический курс ML состоит из описания классов моделей и способов работы с ними. Несмотря на то, что для решения большинства практических задач на сегодня достаточно знать только два типа моделей – **градиентный бустинг на решающих деревьях** и **нейросетевые модели** – мы постараемся рассказать и про другие, чтобы развить у вас глубинное понимание предмета и дать возможность не только использова
