---
title: Метрические методы
author: dmitry_norkin
---

{:.no_toc}

- Этот список будет заменен оглавлением, за вычетом заголовка "Contents",
  к которому добавлен класс `no_toc`.
  {:toc}

Смысл метрических методов очень хорошо раскрывает поговорка "Скажи мне, кто твой друг, и я скажу, кто ты". Алгоритмы этого класса почти не имеют фазы трейнинга. Вместо этого они просто запоминают всю обучающую выборку, а на фазе инференса просто ищут похожие на целевой объекты. Такие методы называют lazy-learning, потому что никакого обучения по сути не происходит. Также они являются непараметрическими, потому что они не делают явных допущений о распределении данных, в отличие от параметрических, которые предполагают, что данные соответствуют некоторому распределению из заданного параметрического семейства. Эти свойства могут быть полезными особенно в случае нелинейных данных, однако с другой стороны из-за lazy-learning алгоритм становится абсолютно неприменимым при большом количестве данных. Несмотря на то, что эти алгоритмы очень просты для понимания, они являются довольно точными и часто используются как минимум в качестве бейзлайнов в разных задачах.

Обзору одного из самых известных метрических алгоритмов -- методу _k-ближайших соседей_ или _k-nearest neighbors (KNN)_, будет посвящена первая часть главы. Этот подход в основном чисто инженерный из-за отсутствия фазы обучения, и в настоящее время уже почти нигде не применяется, однако многие техники, на которых основан алгоритм, используются и в других методах. Например, алгоритмы поиска ближайших соседей, которые являются неотъемлемой частью метода, имеют намного более широкую область применения. Плюс ко всему, KNN --- очень простой и легко интерпретируемый алгоритм, поэтому изучить его все равно полезно. Мы обсудим подробнее его преимущества, недостатки, область его применения, а также возможные обобщения.

Для метрических методов очень важно уметь эффективно находить ближайшие объекты, поэтому задача их поиска неизбежно возникает при применении любого такого алгоритма. Возможные решения этой проблемы мы рассмотрим во второй части главы.

&nbsp;

# Метод k-ближайших соседей (KNN)

Представим, что мы проводим классификацию объектов на два класса --- красный или зеленый. Нам дана некоторая обучающая выборка, и целевой объект --- звездочка:

![](imgs/knn.png){: .center style="width: 30vw"}

Мы хотим классифицировать звездочку на один из двух классов. Интуитивно очевидно, что звездочка должна быть зеленой, потому что она как бы располагается в зеленом кластере, то есть все ее соседи зеленые. Эта интуиция и отражает суть метода KNN --- классифицировать целевой объект исходя из того, какие классы у объектов, которые максимально похожи на него.\

Перейдем теперь к более формальному описанию алгоритма. Рассмотрим сначала задачу классификации на $C$ классов. Пусть дана обучающая выборка $X = (x_i, y_i)_{i=1}^l$, где $x_i \in \mathbb{X}, \ y_i \in \mathbb{Y}=\{1,\ldots,C\}$. Пусть также задана некоторая симметричная по своим аргументам функция расстояния $\rho : \mathbb{X} \times \mathbb{X} \to [0, +\infty)$. Предположим, что требуется классифицировать новый объект $u$. Для этого найдем $k$ наиболее близких к $u$ в смысле расстояния $\rho$ объектов обучающей выборки $X_u(k) = \{x^{(1)}_u,\ldots,x^{(k)}_u\}$:

$$
\forall x_{\rm in}\in X_u(k) \ \forall x_{\rm out} \in X \setminus X_u(k) \quad \rho(u, x_{\rm in}) \leqslant \rho(u, x_{\rm out}). \tag{1}
$$

Метку класса объекта $x^{(i)}_u$ будем обозначать $y_u^{(i)}$. Класс нового объекта тогда естественным образом определим как наиболее часто встречающийся класс среди объектов из $X_u(k)$:

$$
a(u) = \underset{y\in \mathbb{Y}}{\operatorname{arg max}} \sum_{i=1}^k [y_u^{(i)} = y]. \tag{2}
$$

Формула может показаться страшной, но на самом деле все довольно просто: для каждой метки класса $y \in \mathbb{Y}$ количество соседей $u$ с такой меткой можно посчитать просто просуммировав по всем соседям индикаторы событий, соответствующих тому, что метка соседа равна $y$.

Легко заметить, что этот алгоритм позволяет также оценивать вероятности классов. Для этого достаточно просто посчитать частоты классов соседей:

$$
\mathbb{P}(y) = \frac{\sum_{i=1}^k [y_u^{(i)} = y]}k
$$

Стоит, однако, понимать, что хоть такая функция и удовлетворяет свойствам вероятности (она неотрицательна, аддитивна и ограничена единицей), это есть не более чем эвристика.

Несмотря на то, что формально фаза обучения отсутствует, алгоритм может легко переобучиться. Вы можете убедиться в этом сами, использовав маленькое количество соседей (например 1 или 2) --- границы классов оказываются довольно сложными. Происходит это из-за того, что параметрами алгоритма можно считать всю обучающую выборку, которая довольно большая по размеру. Из-за этого алгоритму легко подогнаться под конкретные данные.

{% include_relative knn_clf/knn_clf.html %}

<p style="color:darkslategrey;font-size:14px">
  Автор интерактивной вставки:{' '}
  <a href="mailto:Aechirikova@edu.hse.ru">Чирикова Анастасия</a>.
</p>

&nbsp;

## Выбор метрики

Может возникнуть закономерный вопрос, как же правильно выбрать функцию расстояния $\rho$. В подавляющем большинстве случаев обычное евклидово расстояние $\rho(x, y) = \sqrt{\sum_i (x_i - y_i)^2}$ будет хорошим выбором. Однако в некоторых случаях другие функции будут подходить лучше, поэтому давайте разберем еще несколько функций, наиболее используемых на практике.

![](./imgs/distances.jpg){: .center style="width: 500px"}

<br />

- **Манхэттэнская метрика**

$$
\rho(x, y) = \sum_i \vert x_i - y_i \vert
$$

Часто используется в высокоразмерных пространствах из-за лучшей устойчивости к выбросам. Представим, что два объекта в 1000-размерном пространстве почти идентичны, но сильно отличаются по одному из признаков. Это почти наверняка свидетельствует о выбросе в этом признаке, и объекты скорее всего очень близки. Однако евклидово расстояние усилит различие в единственном признаке и сделает их более далекими друг от друга, в отличие от манхэттэнской метрики, в которой используется модуль вместо квадрата.

- **Метрика Минковского**

$$
\rho(x, y) = \left(\sum_i (x_i - y_i)^p\right)^{1/p}
$$

Является обобщением евклидовой ($p=2$) и манхэттэнской ($p=1$) метрик.

- **Косинусное расстояние**

$$
\rho(x,y) = 1 - \cos \theta = 1 - \frac{x \cdot y}{\|x\| \|y\|}
$$

Эта метрика хороша тем, что не зависит от норм векторов. Такое поведение бывает полезно в некоторых задачах, например при поиске похожих документов. В качестве признаков там часто используются количества слов. При этом, интуитивно кажется, что если в тексте использовать каждое слово в два раза больше, то тема этого текста поменяться не должна. Поэтому как раз в этом случае нам не важна норма вектор-признака, и в задачах связанных с текстами часто применяется именно косинусное расстояние.

- **Расстояние Жаккарда**

$$
\rho(A, B) = 1 - \frac{|A\cap B|}{|A\cup B|}
$$

Его стоит использовать, если исследуемые объекты это некоторые множества. Это полезно тем, что нет нужды придумывать векторные представления для этих множеств, чтобы использовать традиционные метрики.

Вообще говоря, несмотря на некоторые эвристические соображения по выбору метрики, ее можно считать гиперпараметром и подбирать соответствующими способами. Часто качество модели сильно зависит от выбора метрики, а иногда выбрать правильную метрику очень тяжело. Например в случае, когда данные имеют сильно разный масштаб, выбрать подходящую метрику почти невозможно, и нужно сперва проводить нормализацию.

&nbsp;

## Обобщения алгоритма

### Взвешенный KNN

У оригинального алгоритма есть один большой недостаток: он никак не учитывает расстояния до соседних объектов, хотя эта информация может быть полезной.

Давайте попробуем придумать, как исправить этот недостаток. Нам нужно каким-то образом увеличивать вклад близких объектов и уменьшать вклад далеких. Можно заметить, что все индикаторы в формуле $(2)$ учитываются в сумме с одинаковыми коэффициентами. Возникает идея --- назначить этим индикаторам веса, которые тем больше, чем ближе объект к целевому. Таким образом, получаем следующую формулу:

$$
a(u) = \underset{y\in \mathbb{Y}}{\operatorname{arg max}} \sum_{i=1}^k w_i[y_u^{(i)} = y]. \tag{3}
$$

Такой алгоритм называется _взвешенный KNN_ (_weighted KNN_).

Есть множество вариантов выбора весов для объектов, которые можно поделить на две большие группы. В первой группе веса зависят лишь от порядкового номера объекта в отсортированном по близости к $u$ массиве $X_k(u)$. Чаще всего берутся линейно $\left( w_i = \frac{k+1-i}{k} \right)$ или экспоненциально $\left( w_i = q^i, \ 0 < q < 1\right)$ затухающие веса.

Однако здесь мы также не используем всю информацию, которая нам доступна. Зачем использовать порядок соседей, порождаемый расстояниями, если можно использовать сами расстояния? Во второй группе методов вес является некоторой функцией от расстояния. Давайте подумаем, какие должны быть свойства у этой функции. Очевидно, она должна быть положительной на своей области определения, иначе модель будет поощрять несовпадение с некоторыми ближайшими соседями. Также необходимо, чтобы функция монотонно не возрастала, чтобы вес близких соседей был больше, чем далеких. Таким образом вводится так называемая _ядерная функция_ (_kernel function_) $K : \mathbb{R} \to \mathbb{R}$, обладающая перечисленными выше свойствами, с помощью которой и высчитывается вес каждого соседа:

$$
a(u) = \underset{y\in \mathbb{Y}}{\operatorname{arg max}} \sum_{i=1}^k K\left(\frac{\rho(u, x_u^{(i)})}{h}\right)[y_u^{(i)} = y], \tag{4}
$$

где $h$ --- некое положительное число, которое называется _шириной окна_.

От выбора ядра зависит гладкость аппроксимации, но на ее качество этот выбор почти не влияет. Примеры ядерных функций в порядке увеличения их гладкости:

- $K(x) = \left[ \vert x \vert \leqslant 1\right]$ --- прямоугольное ядро;

- $K(x) = \left(1 - \vert x \vert \right)\left[\vert x \vert \leqslant 1\right]$ --- треугольное ядро (непрерывное);

- $K(x) = \left(1 - x^2\right) \left[ \vert x \vert \leqslant 1\right]$ --- ядро Епанечникова (гладкое везде кроме -1 и 1);

- $K(x) = \left(1 - x^2\right)^2\left[\vert x \vert \leqslant 1\right]$ --- квартическое ядро (гладкое везде);

- $K(x) = e^{-2x^2}$ --- гауссово ядро (бесконечное гладкое везде).

На практике чаще всего используют либо прямоугольное для простоты, либо гауссово, в случае когда важна гладкость модели (немного забегая вперед --- это особенно важно в регрессии).

Ширина окна, в свою очередь, сильно влияет как раз на качество модели. При слишком маленькой ширине модель сильно подстраивается под обучающую выборку и теряет свою обобщающую способность. При слишком большой ширине, напротив, модель становится слишком простой. Универсальной ширины окна не существует, поэтому для каждой задачи ее приходится подбирать отдельно.

### Kernel regression

Алгоритм KNN можно довольно легко обобщить и на задачу регрессии. Самые очевидные способы --- брать либо обычное среднее:

$$
a(u) = \frac{\sum_{i=1}^k y_u^{(i)}}{k}, \tag{5}
$$

либо взвешенный вариант:

$$
a(u) = \frac{\sum_{i=1}^k K\left(\frac{\rho(u, x_u^{(i)})}{h}\right) y_u^{(i)}}{\sum_{i=1}^k K\left(\frac{\rho(u, x_u^{(i)})}{h}\right)}. \tag{6}
$$

Последняя формула называется _формулой Надарая-Ватсона_, которая является одним из непараметрических методов восстановления условного матожидания случайной величины, объединенных названием _ядерная регрессия_ (_kernel regression_).

Выписать ответ, конечно, просто, но возникает интересный вопрос: можно ли использовать оптимизационные формулы из задачи классификации? Сначала давайте подумаем, что выдаст алгоритм, если формулу $(4)$ применить без изменений. В задаче регрессии почти наверное все значения $y_u^{(i)}$ будут различными. Поэтому для любого $y$ сумма в формуле $(4)$ будет состоять из не более чем одного слагаемого, а значит максимум будет достигаться на соседе с наибольшим весом, то есть на ближайшем соседе. Это означает, что метод всегда вырождается в 1-NN. Это не совсем то, чего мы добиваемся, поэтому давайте немного модифицируем алгоритм.

Давайте сперва подумаем, а для чего вообще в формуле $(4)$ используется индикатор. В задаче классификации индикатор --- естественная мера близости двух объектов: если объекты совпадают, то значение $1$, если различаются, то $0$. Проблема в том, что в задаче регрессии объекты являются действительными числами, и для них функция, которая выдает отличное от нуля значение лишь в одной точке $y=y_i$ --- плохая мера близости. В случае непрерывных значений $y$ естественно использовать более гладкие функции для выражения близости. Таким образом для обобщения формулы $(4)$ на задачу регрессии нам необходимо всего лишь заменить индикатор на некоторую более гладкую функцию. При этом для действительных чисел чаще всего рассматривают не близость, а расстояние между ними, то есть некоторую метрику. Например, в качестве такой метрики можно взять квадрат евклидова расстояния $(y- y_u^{(i)})^2$. Отметим, что максимизация близости эквивалентна минимизации расстояния, и получим следующую формулу:

$$
a(u) = \underset{y\in \mathbb{R}}{\operatorname{arg min}} \sum_{i=1}^k K\left(\frac{\rho(u, x_u^{(i)})}{h}\right)(y- y_u^{(i)})^2. \tag{7}
$$

Выбор именно этой функции хорош тем, что у этой оптимизационной задачи есть точное решение, и оно записывается как раз формулой $(6)$.

Для ядерной регрессии справедливы те же рассуждения про выбор ядра и ширины окна, которые были приведены в прошлом разделе про классификацию.

Влияние ширины окна и вида ядра на вид функции:
![](./imgs/knn_reg_gaus.png){: .center style="width:30vw"}
![](./imgs/knn_reg_triang.png){: .center style="width:30vw"}
![](./imgs/knn_reg_square.png){: .center style="width:30vw"}
([источник картинки](https://www.dropbox.com/s/quipn5ixsqp3sbv/slides-Metric17.pdf?dl=0))

&nbsp;

## Преимущества и недостатки

Сперва поговорим о преимуществах алгоритма.

- Непараметрический, то есть не делает явных предположений о распределении данных;

- Очень простой в объяснении и интерпретации;

- Достаточно точный, хоть и чаще всего уступает градиентному бустингу и случайному лесу в accuracy;

- Может быть использован как для классификации, так и для регрессии.

Несмотря на большие преимущества, алгоритм не лишен и минусов.

- Неэффективный по памяти, поскольку нужно хранить всю обучающую выборку;

- Вычислительно дорогой по той же причине;

- Чувствителен к масштабу данных, а также к неинформативным признакам.

- Для применения алгоритма необходимо, чтобы метрическая близость объектов совпадала с их семантической близостью, чего не всегда просто добиться. Представим, например, что мы решаем задачу нахождения похожих изображений. Мы хотим, чтобы картинки с лесом находились близко друг к другу, однако если взять любую попиксельную метрику, такие картинки могут быть очень далеки друг от друга. Зачастую для решения этой проблемы вначале обучают представления или делают _metric learning_.

&nbsp;

## Применения

Из-за своих недостатков алгоритм очень неэффективен в задачах с большим количеством данных. Однако у него все равно есть много применений в реальном мире. Приведем лишь некоторые из них:

- Рекомендательные системы. Если посмотреть на саму формулировку задачи "предложить пользователю что-то похожее на то, что он любит", то KNN прямо напрашивается в качестве решения. Несмотря на то, что сейчас часто используются более совершенные алгоритмы, метод ближайших соседей все равно применяется в качестве хорошего бейзлайна.

- Поиск семантически похожих документов. Если векторные представления близки друг к другу, то темы документов схожи.

- Поиск аномалий и выбросов. Из-за того, что алгоритм запоминает обучающую выборку полностью, ему легко посмотреть, насколько целевой объект похож на все данные, которые он видел.

- Задача кредитного скоринга. Рейтинги двух людей, у которых примерно одинаковая зарплата, схожие должности и кредитные истории, не должны сильно отличаться, поэтому KNN отлично подходит для решения такой задачи.

Вопрос сложности алгоритма не очевиден, и требует детального анализа, который будет частично проведен в следующей подглаве.

&nbsp;

# Поиск ближайших соседей

Для того, чтобы применять метод ближайших соседей, нужно уметь как-то находить этих самых соседей. С первого взгляда может показаться, что никакой проблемы нет --- действительно, можно ведь просто перебрать все объекты из обучающей выборки $X = (x_i, y_i)_{i=1}^{\ell}$, посчитать для каждого из них расстояние до тестового объекта и затем найти минимум. Однако, несмотря на то, что сложность такого поиска линейная по $\ell$, она также зависит и от размерности пространства признаков. Если $x \in \mathbb{R}^d$, то сложность такого алгоритма поиска $O(\ell d)$. Если вспомнить, что в типичной задаче машинного обучения количество признаков $d$ может быть порядка $100$, а размер выборки и вовсе может исчисляться десятками и сотнями тысяч объектов, то становится ясно, что такая сложность никуда не годится. Проблема осложняется еще и тем, что данный поиск необходимо выполнять на этапе инференса, который должен быть быстрым. Все это означает, что возникает необходимость в более быстрых методах поиска ближайших соседей, чем простой перебор.

Все такие методы можно поделить на две основные группы: точные и приближенные. Последние, как следует из их названия, находят соседей лишь приближенно, то есть найденные объекты хоть и будут действительно близки, но необязательно будут самыми близкими. В этой подглаве мы подробнее рассмотрим методы из каждой группы.

Перед началом обзора стоит сказать, что хоть мы и рассматриваем алгоритмы поиска соседей именно в контексте их использования в KNN, область их применения значительно шире, и она не ограничивается исключительно машинным обучением. Например, на их основе работает любая информационно-поисковая система, от поиска в Гугле или Яндексе до всем известных алгоритмов Ютьюба.

&nbsp;

# Поиск ближайших соседей: точные методы

Точных методов существует довольно мало. Можно сказать, что их по сути два. Первый --- полный перебор с различными эвристиками. Например, можно выбрать подмножество признаков и считать расстояние только по ним. Оно будет оценкой снизу на реальное расстояние, поэтому если оно уже больше, чем до текущего ближайшего объекта, то можно сразу отбросить этот объект и переходить к следующему. Такие эвристики хоть и могут давать некоторый выигрыш по времени, но не улучшат асимптотическую сложность. Второй --- _k-d деревья_, о которых стоит поговорить подробнее.

## k-d деревья

Представим на секунду, что у нас есть всего лишь один признак, то есть объекты выражаются вещественными числами, а не векторами. В этом случае для поиска ближайшего соседа напрашивается всем вам известное бинарное дерево поиска, которое позволяет находить элементы за логарифмическое время. Оказывается, существует аналог данной структуры в многомерном пространстве, который называется _k-d дерево (k-dimensional tree)_.

Как и в обычном дереве поиска, в k-d дереве каждый узел является объектом обучающей выборки, который особым образом делит пространство на два полупространства. Таким образом, все пространство оказывается поделено на множество малых областей, и такое деление оказывается очень полезным при поиске ближайших соседей.

Рассмотрим подробнее, как строится такое дерево. Трудность в применении обычного дерева поиска состоит в том, что мы не можем напрямую сравнить два вектора так же как два вещественных числа. Чтобы эту проблему преодолеть, узлы дерева будут делить пространство лишь по одной оси. При движении вниз по дереву оси, по которым точки делят пространство, циклически сменяют друг друга. Например, в двумерном пространстве корень будет отвечать за деление по x-координате, его сыновья --- за деление по y-координате, а внуки --- снова за x-координату, и так далее. Посмотрим, как это работает на примере:

![](./imgs/kdd_1.png){: .center}

([источник картинки](https://www.cs.cmu.edu/~ckingsf/bioinfo-lectures/kdtrees.pdf))

На картинке выше корень $(30, 40)$ делит все точки по оси х: слева оказываются точки, у которых $x < 30$, а справа --- те, у которых $x\geqslant 30$. Аналогично, левый сын корня $(5, 25)$ делит свое поддерево по оси y: слева оказываются точки, у которых $y < 25$, а справа --- те, у которых $y\geqslant 25$.

Остается вопрос --- как выбирать точки, которые будут делить пространство пополам? Чтобы дерево было сбалансированным нужно находить точку с медианой соответствующей уровню поддерева координаты. На практике часто ограничиваются выбором случайной точки или любой эвристикой по приближенному поиску медианы (например, медиана некоторого подмножества точек). Это позволяет ускорить построение дерева, но убирает все гарантии на его сбалансированность.

Добавлять новые точки можно так же, как и в одномерном дереве поиска. Спускаясь по дереву, можно однозначно определить лист, к которому нужно подвесить новую точку, чтобы не нарушить все свойства дерева. При добавлении большого количества точек, однако, дерево может перестать быть сбалансированным, и нужно проводить ребалансировку. Также существуют варианты k-d деревьев, которые сохраняют сбалансированность при добавлении/удалении точек.

Поговорим теперь про то, как же находить ближайших соседей с помощью такого дерева. Будем производить обход дерева в глубину с двумя модификациями. Во-первых, будем запоминать наиболее близкую точку. Это позволит не заходить в поддеревья, задающие области, которые заведомо дальше, чем текущая наиболее близкая точка, поэтому не имеет смысла искать в нем ближайших соседей. Во-вторых, будем прежде всего обходить те поддеревья, которые задают наиболее близкие области, а значит, с большей вероятностью содержат ближайшего соседа.

![](./imgs/kdd_apply.gif){: .center}

([источник картинки](https://commons.wikimedia.org/))

Сложность метода по размеру обучающей выборки в среднем равна $O(\log \ell)$ при равномерном распределении точек. При большой размерности пространства, однако, алгоритму приходится посещать больше ветвей дерева, чтобы найти ближайших соседей. Например, если $\ell \approx d$, то сложность становится примерно такой же, как и в случае полного перебора. В общем случае считается, что для того, чтобы асимптотика действительно была логарифмической, нужно, чтобы $\ell \gtrsim 2^d$. Поэтому уже при количестве признаков порядка сотни алгоритм не дает существенных преимуществ перед полным перебором.

- [Хорошая презентация](https://www.cs.cmu.edu/~ckingsf/bioinfo-lectures/kdtrees.pdf), объясняющая структуру и поиск соседей

- [Балансировка деревьев](https://en.wikipedia.org/wiki/K-d_tree#Balancing)

&nbsp;

# Поиск ближайших соседей: приближенные методы

Почти всегда находить именно самых близких соседей необязательно. Например, в задаче подбора рекомендаций фильмов пользователю чаще всего не нужны наиболее похожие картины, достаточно, к примеру, 10 из 15 наиболее близких. Поэтому, чтобы ускорить процесс поиска соседей, используют приближенные методы. Разберем основные идеи, которые применяются в таких методах.

## Random projection trees

Алгоритмы, основанные на деревьях очень часто применяются в задачах поиска соседей. Идея всех таких методов заключается в итеративном разделении пространства случайными гиперплоскостями и построения дерева на базе этого разделения, в листах которого содержится малое число объектов.

Одним из наиболее ярких представителей этого семейства является _Annoy_ --- алгоритм, который используется Spotify для рекомендаций музыки. Задача подобных рекомендательных систем довольно простая --- нужно посоветовать пользователю композиции, которые он еще не слушал, но которые при этом с высокой вероятностью ему понравятся. Простая и рабочая идея --- предлагать композиции, которые похожи на те, которые он уже слушает. Здесь на помощь как раз и приходят методы поиска ближайших соседей.

Annoy в какой-то степени похож на k-d деревья. Сначала выбираются два случайных объекта обучющей выборки и проводится гиперплоскость, симметрично их разделяющая. Затем для каждого полученного полупространства итеративно запускается такая же процедура, которая продолжается до тех пор, пока в каждой области будет не более $M$ объектов ($M$ --- гиперпараметр).

![](./imgs/annoy_structure.gif){: .center style="width: 600px"}

([источник картинки](https://erikbern.com/2015/10/01/nearest-neighbors-and-vector-models-part-2-how-to-search-in-high-dimensional-spaces.html))

Таким образом задается бинарное дерево с глубиной порядка $O(\log \ell)$ в среднем.

![](./imgs/annoy_tree.png){: .center}

([источник картинки](https://erikbern.com/2015/10/01/nearest-neighbors-and-vector-models-part-2-how-to-search-in-high-dimensional-spaces.html))

Cпускаясь по этому дереву, можно найти область, в которой лежит целевой объект и некоторое количество близких к нему элементов обучающей выборки. Проблема в том, что это не обязательно будут самые близкие объекты, поэтому для увеличения точности составляется лес из таких деревьев, и берется объединение соответствующих целевому объекту областей.

![](./imgs/annoy_forest.gif){: .center style="width: 600px"}

([источник картинки](https://erikbern.com/2015/10/01/nearest-neighbors-and-vector-models-part-2-how-to-search-in-high-dimensional-spaces.html))

Чем больше таких деревьев берется, тем более точным будет результат, но придется тратить большее время на его поиск.

Преимуществом алгоритма является простота нахождения трейдоффа между скоростью работы и точностью с помощью тюнинга гиперпараметров. К минусам можно отнести то, что алгоритм плохо параллелится и переносится на GPU, не работает эффективно с батчами, а также, что для добавления новой точки в обучающую выборку придется перезапускать процедуру с самого начала.

- [Отличная статья](https://erikbern.com/2015/10/01/nearest-neighbors-and-vector-models-part-2-how-to-search-in-high-dimensional-spaces.html) с иллюстрациями и подробным описанием алгоритма

## Locality sensitive hashing (LSH)

Предположим, что мы можем построить такую хэш-функцию, которая переводит близкие объекты в один бакет. Тогда близких соседей целевого объекта можно найти посчитав его хэш, и посмотрев на коллизии. Оказывается, такие хэш-функции существуют, и на этой идее основано несколько алгоритмов, которые объединяются названием _Locality sensitive hashing (LSH)_. К этому классу алгоритмов относится, например, _Faiss_, используемый Facebook.

![](./imgs/lsh.png){: .center}

([источник картинки](https://brc7.github.io/2019/09/19/Visual-LSH.html))

Определим формально семейство хэш-функций, которое мы хотим использовать. Нам нужно, чтобы вероятность коллизии на близких объектах была высокая, а на далеких --- низкая. Назовем семейство хэш-функций $\mathcal{H}$ $(R, cR, p_1, p_2)$-чувствительным, если для любой $h(x)\in\mathcal{H}$:

- Для $\rho(x, y) < R$ вероятность коллизии $\Pr\left[h(x)=h(y)\right] > p_1$

- Для $\rho(x, y) > cR$ вероятность коллизии $\Pr\left[h(x)=h(y)\right] < p_2$

Формулы могут выглядеть сложными, но это всего лишь формализация нашей интуиции. Картинка ниже поясняет определение: для близких красных объектов в шаре радиусом $R$ вероятность коллизии больше $p_1$, для далеких синих объектов на расстоянии больше $cR$ вероятность коллизии меньше $p_2$, а про серые объекты в слое между $R$ и $cR$ мы ничего не знаем.

![](./imgs/lsh_hash.png){: .center style="width: 500px"}

([источник картинки](https://brc7.github.io/2019/09/19/Visual-LSH.html))

Для каждой функции расстояния, используемой в задаче, существует свое подходящее семейство хэш-функций. Например, для евклидовой и манхэттэнской метрик используются _случайные проекции_, где хэш-функция имеет следующий вид:

$$
h\_{\boldsymbol{w}, b}(\boldsymbol{x}) = \left\lfloor \frac{\boldsymbol{w}^T\boldsymbol{x} + b}{r}\right\rfloor,
$$

где $\boldsymbol{w}$ и $b$ --- случайные параметры, а $r$ выбирается пользователем. $b$ выбирается равномерно из отрезка $[0, r]$, а $\boldsymbol{w}$ генерируется либо из нормального распределения, что соответствует евклидовой метрике, либо из распределения Коши --- для манхэттэнской метрики.

По сути такая функция разбивает все пространство на слои в направлении вектора $\boldsymbol{w}$. Параметр $r$ при этом задает ширину слоя.

![](./imgs/lsh_splits.png){: .center style="width: 500px"}

([источник картинки](https://brc7.github.io/2019/09/19/Visual-LSH.html))

На практике при использовании лишь одной хэш-функции разница между $p_1$ и $p_2$ оказывается очень маленькой, поэтому применяют различные методы для ее увеличения. Первый способ --- уменьшать размер бакетов в хэш-таблице путем использования композиции разных хэш-функций из одного семейства $g(x) = (h_1(x),\ldots,h_m(x))$. Преимущество этого способа как раз хорошо видно на примере случайных проекций. При использовании лишь одной хэш-функции бакетами являются слои бесконечного объема. Однако при использовании композиции размером как минимум равным количеству признаков $d$, то из-за случайности выбора вектора $\boldsymbol{w}$, бакеты почти наверное станут замкнутыми фигурами с конечным объемом. Второй способ повышения эффективности алгоритма --- использовать несколько хэш-таблиц и искать соседей среди коллизий в каждой из них. На практике используют оба метода сразу, подбирая $m$ и $L$ --- количество хэш-таблиц, как гиперпараметры.

К плюсам алгоритма можно отнести хорошие теоретические гарантии на сублинейное время и, как и в Annoy, простой поиск трейдоффа между точностью и скоростью работы. Минусами можно назвать высокую потребность в памяти, плохая адаптируемость под GPU, а также, тот факт, что, несмотря на теоретические гарантии в среднем, на практике алгоритм может работать даже чуть дольше полного перебора из-за того, что помимо самого поиска требуется искать хэши объектов.

- [Отличная статья](https://randorithms.com/2019/09/19/Visual-LSH.html) с объяснением в иллюстрациях и примерами хэш-функций для других метрик;

- [Еще одна статья](https://towardsdatascience.com/understanding-locality-sensitive-hashing-49f6d1f6134), в которой шаг за шагом выводится алгоритм на примере расстояния Жаккарда.

## Proximity graphs & Hierarchical Navigable Small World (HNSW)

Следующий класс алгоритмов основан на построении специального _графа близости_ (_proximity graph_) на объектах выборки и дальнейшем жадном поиске по этому графу. Алгоритмы этого семейства сейчас считаются state-of-the-art для многих задач.

Рассмотрим подробнее этот класс алгоритмов на примере одного из наиболее популярных из них под названием _Navigable Small World (NSW)_. Идея его в следующем: на данных строится граф (он также называется NSW), который удовлетворяет двум следующим свойствам:

1. Между любыми двумя точками существует короткий путь или, более формально, матожидание числа кратчайшего пути между двумя случайно выбранными вершинами растет как $O(\log\ell)$;

2. Средняя степень вершины мала.

На первый взгляд может показаться, что тяжело выполнить одновременно оба свойства, но на самом деле большая часть графов в реальном мире являются NSW графами. Самый простой пример --- это известное "правило шести рукопожатий": любые два случайных человек соединены короткой последовательностью личных контактов длиной не более 6, несмотря на то, что количество знакомых у среднего человека ($100$-$1000$) мало по сравнению с населением Земли.

В таких графах существует очень простой метод поиска соседей. Нужно выбрать случайную точку, среди ее соседей выбрать того, который ближе всего к целевому объекту и повторить процедуру уже для него. Показано, что такой жадный поиск имеет полилогарифмическую асимптотику.

![](./imgs/hnsw_graph.png){: .center style="width: 400px"}

([источник картинки](https://opendistro.github.io/for-elasticsearch/blog/odfe-updates/2020/04/Building-k-Nearest-Neighbor-%28k-NN%29-Similarity-Search-Engine-with-Elasticsearch/))

Проблема такого подхода в том, что можно попасть в плотный кластер и очень долго оттуда выбираться. Для решения этой проблемы используется иерархия NSW или _Hierarchical Navigable Small World (HNSW)_. Исходный граф является нулевым слоем. Каждый следующий слой строится в два шага:

1. Каждая вершина текущего слоя попадает в следующий с некоторой вероятностью $p$.

2. На всех вершинах, попавших в новый слой, строится NSW.

По построению количество слоев будет $O(\log\ell)$.

![](./imgs/hnsw_layers.jpg){: .center style="width: 400px"}

([источник картинки](https://arxiv.org/abs/1603.09320))

Поиск начинается в самом верхнем слое. После нахождения ближайшей к целевому объекту вершины, спускаемся на слой ниже и начинаем поиск из этой вершины. Повторяем процедуру, пока не спустимся до нулевого слоя. Таким образом, на каждом слое мы все больше уточняем наш ответ. Стоит отметить, что для ускорения работы иногда поиск останавливают не при нахождении ближайшей вершины, а раньше, используя критерии остановки.

Интуитивно легко понять, почему такая иерархическая структура решает проблему плотных кластеров: в верхних слоях вершин мало, а расстояния между ними в среднем большие, а значит таких кластеров там почти нет. Поэтому, попадая в нижний слой, мы чаще всего оказываемся уже в нужном кластере и просто уточняем результат работы алгоритма.

HNSW, так же как и рассмотренные ранее приближенные методы, позволяет искать трейдофф между точностью и скоростью работы. Плюс ко всему, на реальных данных он часто работает лучше других методов и сейчас считается SotA. Однако, этот способ поиска не лишен и недостатков. Главный заключается в том, что нельзя добавлять точки в обучающую выборку без перестройки структуры. Помимо этого, он довольно требователен по памяти из-за того, что для каждого слоя приходится хранить как вершины, которые в него входят, так и связи между этими вершинами.

- [Оригинальная статья](https://arxiv.org/abs/1603.09320)

---

В завершение стоит сказать, что не существует универсального метода поиска соседей --- каждый из описанных методов может быть лучше других в определенной задачи. К тому же, несмотря на то, что приближенные методы имеют лучшую асимптотику, многие из них плохо переносятся на GPU. Из-за этого на практике полный перебор бывает быстрее любого из таких приближенных методов.
